# Attention Is All You Need

## Introduction:
This paper is a classic articles in deep learning. It first introduce the Transformer structure, which has become the basis for most models. People often chose recurrent structure such as RNN to handle sequence-to-sequence or NLP problems. But Transformer firstly use self-attention and use Encoder-Decoder structure to do.

## What direction does this paper belong to ? 
Transformer was first used in NLP and Seq2Seq problems. Transformer is the basic structure of most models today.  
ViT: vision Transformer is an extention of Transformer and ViT is widely used in computer vision.

## What problems does this paper solve ?  
1. Before the emergence of Transformer, people always use recurrent structures and hadn't utilized attention mechanism. But recurrent structure could significant computational problems. They were cost of time and moeny. Transformer adopts Encoder-Decoder structure to solve this problems.

2. Before the emergence of Transformer, NLP and Seq2Seq could hardly catch the information of context. Eventhough LSTM and GRU could allenviate this problems to some extent, but there has never been a good solution. Transformer use self-attention mechanism to largely solved this problems.

## Why can this method solve this problems ?
1. Use Encoder-Decoder sturcture
2. Use self-Attention mechnism.

## What should we do next? 
This paper is a classic paper in deep learning. Transformer has already form the basic structure of deep learning. Transfromer was proposed in 2017 and it's been 7 years now. Today 2024.9.3 They still use Transformer as a basic structure. As we continue to improve the Transformer model, we also need to constantly explore new architecture.

## Conclusion:
In the past few years, LLMs could be developed by the scale of computation.That's the reason why ChatGPT could make a huge success and why GPT4 have so amazing function.  

We could see NVIDIA get to the center of the world and get a huge success. That's because they could make up GPU which is used in AI.  

Every coin have two sides, NVIDIA has made a signicant progress in the AI industry by leveraging advanced GPU chips. But the huge function of GPU also prevent the progress of the model structure.Almost every company choose to generate LLMs by use the high performance GPU, they ignore the progress of model GPU. We could easily see that Transformer structure is still basic structure today eventhough it has introduced for almost 7 years.  

People will face the chanllenge soon when the function of GPU couldn't develop the AI product.And at that time, the model structure design could go to the center of the world again. We all know that how much development the application of convolution has brought to AI, so the next explosive advancement in AI development is likely to be the application of new mathmatical methods.
Let's look for the next great time !  