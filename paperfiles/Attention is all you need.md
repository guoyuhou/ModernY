# Attention Is All You Need

## Introduction:
This paper is a classic articles in deep learning. It first introduce the Transformer structure, which has become the basis for most models. People often chose recurrent structure such as RNN to handle sequence-to-sequence or NLP problems. But Transformer firstly use self-attention and use Encoder-Decoder structure to do.

## What direction does this paper belong to ? 
Transformer was first used in NLP and Seq2Seq problems. Transformer is the basic structure of most models today.  
ViT: vision Transformer is an extention of Transformer and ViT is widely used in computer vision.

## What problems does this paper solve ?  
1. Before the emergence of Transformer, people always use recurrent structures and hadn't utilized attention mechanism. But recurrent structure could significant computational problems. They were cost of time and moeny. Transformer adopts Encoder-Decoder structure to solve this problems.

2. Before the emergence of Transformer, NLP and Seq2Seq could hardly catch the information of context. Eventhough LSTM and GRU could allenviate this problems to some extent, but there has never been a good solution. Transformer use self-attention mechanism to largely solved this problems.

## Why can this method solve this problems ?
1. Use Encoder-Decoder sturcture
2. Use self-Attention mechnism.

## What should we do next? 
This paper is a classic paper in deep learning. Transformer has already form the basic structure of deep learning. Transfromer was proposed in 2017 and it's been 7 years now. Today 2024.9.3 They still use Transformer as a basic structure. As we continue to improve the Transformer model, we also need to constantly explore new architecture.